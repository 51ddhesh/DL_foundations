{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4bc3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7917b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    '''\n",
    "        Generate positional encoding for input sequences\n",
    "    '''\n",
    "    pe = np.zeros((seq_len, d_model)) # Positional encoding matrix\n",
    "    position = np.arange(0, seq_len)[:, np.newaxis] \n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d593d05",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "    Attention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e4018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    '''\n",
    "        Compute the scaled dot-product attention\n",
    "    '''\n",
    "    d_k = query.shape[-1]\n",
    "    scores = np.matmul(query, key.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    attention_output = np.matmul(attention_weights, value)\n",
    "    return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c334962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_q = np.random.randn(d_model, d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        Q = np.matmul(x, self.W_q).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        K = np.matmul(x, self.W_k).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        V = np.matmul(x, self.W_v).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n",
    "        output = np.matmul(output, self.W_o)\n",
    "        return output, weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
