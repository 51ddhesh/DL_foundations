{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4bc3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7917b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    '''\n",
    "        Generate positional encoding for input sequences\n",
    "    '''\n",
    "    pe = np.zeros((seq_len, d_model)) # Positional encoding matrix\n",
    "    position = np.arange(0, seq_len)[:, np.newaxis] \n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d593d05",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "    Attention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e4018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    '''\n",
    "        Compute the scaled dot-product attention\n",
    "    '''\n",
    "    d_k = query.shape[-1]\n",
    "    scores = np.matmul(query, key.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)\n",
    "    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
    "    attention_output = np.matmul(attention_weights, value)\n",
    "    return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c334962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_q = np.random.randn(d_model, d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        Q = np.matmul(x, self.W_q).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        K = np.matmul(x, self.W_k).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        V = np.matmul(x, self.W_v).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n",
    "        output = np.matmul(output, self.W_o)\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a138496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        self.W1 = np.random.randn(d_model, d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model)\n",
    "        self.b1 = np.zeros((1, d_ff))\n",
    "        self.b2 = np.zeros((1, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.matmul(x, self.W1) + self.b1\n",
    "        x = np.maximum(0, x) # ReLU\n",
    "        x = np.matmul(x, self.W2) + self.b2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d999deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        self.gamma = np.ones((1, 1, d_model))\n",
    "        self.beta = np.zeros((1, 1, d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        x_norm = (x - mean) / np.sqrt(var * self.eps)\n",
    "        return self.gamma * x_norm + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6265887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attention_output, attention_weights = self.mha.forward(x, mask)\n",
    "        x = self.norm1.forward(x + attention_output)\n",
    "        ffn_output = self.ffn.forward(x)\n",
    "        x = self.norm2.forward(x + ffn_output)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e25e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        attention1_output, attention1_weights = self.mha1.forward(x, tgt_mask)\n",
    "        x = self.norm1.forward(x + attention1_output)\n",
    "        attention2_output, attention2_weights = self.mha2.forward(x, src_mask)\n",
    "        x = self.norm2.forward(x + attention2_output)\n",
    "        ffn_output = self.ffn.forward(x)\n",
    "        x = self.norm3.forward(x + ffn_output)\n",
    "        return x, attention1_weights, attention2_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d3c94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len):\n",
    "        self.d_model = d_model\n",
    "        self.encoder_layers = [EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
    "        self.decoder_layers = [DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
    "        self.embedding = np.random.randn(vocab_size, d_model) / np.sqrt(d_model)\n",
    "        self.pos_encoding = positional_encoding(max_seq_len, d_model)\n",
    "        self.final_layer = np.random.randn(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        # Embed and add positional encoding\n",
    "        src_emb = self.embedding[src] + self.pos_encoding[:src.shape[1], :]\n",
    "        tgt_emb = self.embedding[tgt] + self.pos_encoding[:tgt.shape[1], :]        \n",
    "        \n",
    "        # Encoder\n",
    "        enc_output = src_emb\n",
    "        enc_attn_weights = []\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_output, attn_weights = layer.forward(enc_output, src_mask)\n",
    "            enc_attn_weights.append(attn_weights)\n",
    "        \n",
    "        # Decoder\n",
    "        dec_output = tgt_emb\n",
    "        dec_attn_weights1, dec_attn_weights2 = [], []\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_output, attn1_weights, attn2_weights = layer.forward(dec_output, enc_output, src_mask, tgt_mask)\n",
    "            dec_attn_weights1.append(attn1_weights)\n",
    "            dec_attn_weights2.append(attn2_weights)\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = np.matmul(dec_output, self.final_layer)\n",
    "        return output, enc_attn_weights, dec_attn_weights1, dec_attn_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ff4e50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (2, 10, 1000)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    d_model = 64\n",
    "    num_heads = 4\n",
    "    d_ff = 256\n",
    "    num_layers = 2\n",
    "    vocab_size = 1000\n",
    "    max_seq_len = 50\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    \n",
    "    # Dummy input (batch_size, seq_len) with integer token IDs\n",
    "    src = np.random.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    tgt = np.random.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    # Create causal mask for decoder\n",
    "    tgt_mask = np.tril(np.ones((batch_size, 1, seq_len, seq_len)))\n",
    "    \n",
    "    # Initialize and run Transformer\n",
    "    transformer = Transformer(d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len)\n",
    "    output, enc_attn, dec_attn1, dec_attn2 = transformer.forward(src, tgt, tgt_mask=tgt_mask)\n",
    "    \n",
    "    print(\"Output shape:\", output.shape)  # (batch_size, seq_len, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
